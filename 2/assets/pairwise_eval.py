import json
import os
import time
import argparse
from openai import OpenAI
from tqdm import tqdm

# ================== é…ç½®åŒº ==================

# åˆå§‹åŒ– OpenAI å®¢æˆ·ç«¯ï¼ˆå¯¹æ¥ç«å±±æ–¹èˆŸï¼‰
client = OpenAI(
    base_url="https://ark.cn-beijing.volces.com/api/v3",
    api_key=os.environ.get("ARK_API_KEY"),
)

# å¿…é¡»è®¾ç½®ç¯å¢ƒå˜é‡ ARK_API_KEY
if not client.api_key:
    raise ValueError("è¯·è®¾ç½®ç¯å¢ƒå˜é‡ ARK_API_KEY")

# é»˜è®¤ä½¿ç”¨ä½ çš„æ¨ç†æ¥å…¥ç‚¹ ID
MODEL_NAME = "ep-20251008234558-72jvb"  # â† å¯æ›¿æ¢ä¸ºä½ è‡ªå·±çš„ endpoint ID

# è‹±æ–‡è£åˆ¤ Promptï¼ˆæ¨èè‹±æ–‡ï¼Œé¿å…ä¸­æ–‡æŒ‡ä»¤æ··æ·†ï¼‰
JUDGE_PROMPT_EN = """
You are a fair and impartial judge. Compare the two responses below, generated by two different language models, to the same user query. Evaluate which response is better in terms of helpfulness, relevance, accuracy, and clarity.

### User Query:
{prompt}

### Response A (from {model_a}):
{response_a}

### Response B (from {model_b}):
{response_b}

### Instructions:
- If Response A is better, output: [[A]]
- If Response B is better, output: [[B]]
- If both are equally good (or bad), output: [[C]]
- Output only one of: [[A]], [[B]], or [[C]] â€” no explanation.
""".strip()

# ================== å‚æ•°è§£æ ==================
parser = argparse.ArgumentParser()
parser.add_argument('--model_a_name', type=str, required=True)
parser.add_argument('--model_b_name', type=str, required=True)
parser.add_argument('--prompt_file', type=str, required=True)
parser.add_argument('--response_a_file', type=str, required=True)
parser.add_argument('--response_b_file', type=str, required=True)
parser.add_argument('--output_dir', type=str, default="./pairwise_results_ark")
parser.add_argument('--judge_temperature', type=float, default=0.0)
parser.add_argument('--max_retries', type=int, default=3)
parser.add_argument('--delay_between_calls', type=float, default=1.0)
args = parser.parse_args()


# ================== å·¥å…·å‡½æ•° ==================
def ensure_dir(path):
    """ç¡®ä¿ç›®å½•å­˜åœ¨"""
    os.makedirs(path, exist_ok=True)


def read_json(path):
    """è¯»å– JSON æ–‡ä»¶"""
    with open(path, 'r', encoding='utf-8') as f:
        return json.load(f)


def write_json(path, data):
    """å†™å…¥ JSON æ–‡ä»¶"""
    with open(path, 'w', encoding='utf-8') as f:
        json.dump(data, f, indent=2, ensure_ascii=False)


def call_judge(prompt, response_a, response_b, model_a_name, model_b_name):
    """è°ƒç”¨ç«å±±æ–¹èˆŸæ¨¡å‹è¿›è¡Œè£å†³"""
    content = JUDGE_PROMPT_EN.format(
        prompt=prompt,
        response_a=response_a,
        response_b=response_b,
        model_a=model_a_name,
        model_b=model_b_name
    )

    for _ in range(args.max_retries):
        try:
            completion = client.chat.completions.create(
                model=MODEL_NAME,
                messages=[
                    {"role": "system", "content": "You are a helpful assistant."},
                    {"role": "user", "content": content}
                ],
                temperature=args.judge_temperature,
                max_tokens=16
            )
            answer = completion.choices[0].message.content.strip()

            if "[[A]]" in answer:
                return "A"
            elif "[[B]]" in answer:
                return "B"
            elif "[[C]]" in answer:
                return "C"
            else:
                print(f"âš ï¸ Judge output not recognized: {answer}")
                return "ERROR"
        except Exception as e:
            print(f"\nâŒ API Error: {e}, retrying...")
            time.sleep(5)
    return "ERROR"


def load_or_init_results(prompts, responses_a, responses_b, output_dir):
    """åŠ è½½å·²æœ‰ç»“æœæˆ–åˆå§‹åŒ–æ–°ä»»åŠ¡ï¼ˆæ”¯æŒæ–­ç‚¹ç»­è·‘ï¼‰"""
    results_path = os.path.join(output_dir, "pairwise_results.json")
    
    if not os.path.exists(results_path):
        print(f"ğŸ“„ åˆ›å»ºæ–°è¯„ä¼°æ–‡ä»¶: {results_path}")
        results = []
        for i, (p, ra, rb) in enumerate(zip(prompts, responses_a, responses_b)):
            results.append({
                "id": i,
                "prompt": str(p).strip(),
                "response_a": str(ra).strip(),
                "response_b": str(rb).strip(),
                "winner": None
            })
        write_json(results_path, results)
        return results

    print(f"ğŸ” æ£€æµ‹åˆ°å·²æœ‰ç»“æœæ–‡ä»¶ï¼Œæ­£åœ¨åŠ è½½...")
    results = read_json(results_path)

    if len(results) != len(prompts):
        raise ValueError(f"ç»“æœæ–‡ä»¶é•¿åº¦ ({len(results)}) ä¸è¾“å…¥æ•°æ®ä¸ä¸€è‡´ï¼Œè¯·åˆ é™¤æ—§æ–‡ä»¶é‡æ–°å¼€å§‹ã€‚")

    return results


# ================== ä¸»ç¨‹åº ==================
def main():
    ensure_dir(args.output_dir)

    # è¯»å–è¾“å…¥æ•°æ®
    prompts = read_json(args.prompt_file)
    responses_a = read_json(args.response_a_file)
    responses_b = read_json(args.response_b_file)

    assert len(prompts) == len(responses_a) == len(responses_b), "Length mismatch!"

    # åŠ è½½æˆ–åˆå§‹åŒ–ç»“æœ
    results = load_or_init_results(prompts, responses_a, responses_b, args.output_dir)

    total = len(results)
    done_count = sum(1 for r in results if r["winner"] is not None)
    remaining = total - done_count

    print(f"ğŸ“Š æ€»å…± {total} æ¡æ•°æ®")
    print(f"âœ… å·²å®Œæˆ: {done_count}")
    print(f"ğŸ”„ å¾…è¯„ä¼°: {remaining}")

    # ç»Ÿè®¡è®¡æ•°å™¨
    counts = {"A": 0, "B": 0, "C": 0, "ERROR": 0}

    # æ›´æ–°å·²æœ‰çš„è®¡æ•°
    for r in results:
        if r["winner"] in counts:
            counts[r["winner"]] += 1

    # è¿›åº¦æ¡
    for i, item in enumerate(tqdm(results, initial=done_count, total=total, desc="Evaluating")):
        if item["winner"] is not None:
            continue

        winner = call_judge(
            item["prompt"],
            item["response_a"],
            item["response_b"],
            args.model_a_name,
            args.model_b_name
        )

        item["winner"] = winner
        counts[winner] += 1

        # æ¯ 5 æ¡ä¿å­˜ä¸€æ¬¡ï¼Œé˜²æ­¢ä¸­æ–­ä¸¢å¤±
        if (i + 1) % 5 == 0:
            write_json(os.path.join(args.output_dir, "pairwise_results.json"), results)

        time.sleep(args.delay_between_calls)  # æ§åˆ¶è¯·æ±‚é¢‘ç‡

    # ä¿å­˜æœ€ç»ˆç»“æœ
    final_results_path = os.path.join(args.output_dir, "pairwise_results.json")
    write_json(final_results_path, results)

    # ç”Ÿæˆæ‘˜è¦
    total_valid = counts["A"] + counts["B"] + counts["C"]
    win_rate_a = counts["A"] / total_valid if total_valid else 0
    win_rate_b = counts["B"] / total_valid if total_valid else 0

    summary = {
        "model_a": args.model_a_name,
        "model_b": args.model_b_name,
        "total": total,
        "evaluated": total_valid,
        "win_counts": dict(counts),
        "win_rate_A": round(win_rate_a, 4),
        "win_rate_B": round(win_rate_b, 4),
        "tie_rate": round(counts["C"] / total_valid, 4) if total_valid else 0,
        "error_rate": round(counts["ERROR"] / total, 4)
    }

    summary_path = os.path.join(args.output_dir, "summary.json")
    write_json(summary_path, summary)

    print("\n" + "="*60)
    print("âœ… Pairwise Evaluation Completed!")
    print(json.dumps(summary, indent=2, ensure_ascii=False))
    print("="*60)


if __name__ == "__main__":
    main()