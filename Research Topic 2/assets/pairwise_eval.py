import json
import os
import time
import argparse
from openai import OpenAI
from tqdm import tqdm

# ================== 配置区 ==================

# 初始化 OpenAI 客户端（对接火山方舟）
client = OpenAI(
    base_url="https://ark.cn-beijing.volces.com/api/v3",
    api_key=os.environ.get("ARK_API_KEY"),
)

# 必须设置环境变量 ARK_API_KEY
if not client.api_key:
    raise ValueError("请设置环境变量 ARK_API_KEY")

# 默认使用你的推理接入点 ID
MODEL_NAME = "ep-20251008234558-72jvb"  # ← 可替换为你自己的 endpoint ID

# 英文裁判 Prompt（推荐英文，避免中文指令混淆）
JUDGE_PROMPT_EN = """
You are a fair and impartial judge. Compare the two responses below, generated by two different language models, to the same user query. Evaluate which response is better in terms of helpfulness, relevance, accuracy, and clarity.

### User Query:
{prompt}

### Response A (from {model_a}):
{response_a}

### Response B (from {model_b}):
{response_b}

### Instructions:
- If Response A is better, output: [[A]]
- If Response B is better, output: [[B]]
- If both are equally good (or bad), output: [[C]]
- Output only one of: [[A]], [[B]], or [[C]] — no explanation.
""".strip()

# ================== 参数解析 ==================
parser = argparse.ArgumentParser()
parser.add_argument('--model_a_name', type=str, required=True)
parser.add_argument('--model_b_name', type=str, required=True)
parser.add_argument('--prompt_file', type=str, required=True)
parser.add_argument('--response_a_file', type=str, required=True)
parser.add_argument('--response_b_file', type=str, required=True)
parser.add_argument('--output_dir', type=str, default="./pairwise_results_ark")
parser.add_argument('--judge_temperature', type=float, default=0.0)
parser.add_argument('--max_retries', type=int, default=3)
parser.add_argument('--delay_between_calls', type=float, default=1.0)
args = parser.parse_args()


# ================== 工具函数 ==================
def ensure_dir(path):
    """确保目录存在"""
    os.makedirs(path, exist_ok=True)


def read_json(path):
    """读取 JSON 文件"""
    with open(path, 'r', encoding='utf-8') as f:
        return json.load(f)


def write_json(path, data):
    """写入 JSON 文件"""
    with open(path, 'w', encoding='utf-8') as f:
        json.dump(data, f, indent=2, ensure_ascii=False)


def call_judge(prompt, response_a, response_b, model_a_name, model_b_name):
    """调用火山方舟模型进行裁决"""
    content = JUDGE_PROMPT_EN.format(
        prompt=prompt,
        response_a=response_a,
        response_b=response_b,
        model_a=model_a_name,
        model_b=model_b_name
    )

    for _ in range(args.max_retries):
        try:
            completion = client.chat.completions.create(
                model=MODEL_NAME,
                messages=[
                    {"role": "system", "content": "You are a helpful assistant."},
                    {"role": "user", "content": content}
                ],
                temperature=args.judge_temperature,
                max_tokens=16
            )
            answer = completion.choices[0].message.content.strip()

            if "[[A]]" in answer:
                return "A"
            elif "[[B]]" in answer:
                return "B"
            elif "[[C]]" in answer:
                return "C"
            else:
                print(f"⚠️ Judge output not recognized: {answer}")
                return "ERROR"
        except Exception as e:
            print(f"\n❌ API Error: {e}, retrying...")
            time.sleep(5)
    return "ERROR"


def load_or_init_results(prompts, responses_a, responses_b, output_dir):
    """加载已有结果或初始化新任务（支持断点续跑）"""
    results_path = os.path.join(output_dir, "pairwise_results.json")
    
    if not os.path.exists(results_path):
        print(f"📄 创建新评估文件: {results_path}")
        results = []
        for i, (p, ra, rb) in enumerate(zip(prompts, responses_a, responses_b)):
            results.append({
                "id": i,
                "prompt": str(p).strip(),
                "response_a": str(ra).strip(),
                "response_b": str(rb).strip(),
                "winner": None
            })
        write_json(results_path, results)
        return results

    print(f"🔁 检测到已有结果文件，正在加载...")
    results = read_json(results_path)

    if len(results) != len(prompts):
        raise ValueError(f"结果文件长度 ({len(results)}) 与输入数据不一致，请删除旧文件重新开始。")

    return results


# ================== 主程序 ==================
def main():
    ensure_dir(args.output_dir)

    # 读取输入数据
    prompts = read_json(args.prompt_file)
    responses_a = read_json(args.response_a_file)
    responses_b = read_json(args.response_b_file)

    assert len(prompts) == len(responses_a) == len(responses_b), "Length mismatch!"

    # 加载或初始化结果
    results = load_or_init_results(prompts, responses_a, responses_b, args.output_dir)

    total = len(results)
    done_count = sum(1 for r in results if r["winner"] is not None)
    remaining = total - done_count

    print(f"📊 总共 {total} 条数据")
    print(f"✅ 已完成: {done_count}")
    print(f"🔄 待评估: {remaining}")

    # 统计计数器
    counts = {"A": 0, "B": 0, "C": 0, "ERROR": 0}

    # 更新已有的计数
    for r in results:
        if r["winner"] in counts:
            counts[r["winner"]] += 1

    # 进度条
    for i, item in enumerate(tqdm(results, initial=done_count, total=total, desc="Evaluating")):
        if item["winner"] is not None:
            continue

        winner = call_judge(
            item["prompt"],
            item["response_a"],
            item["response_b"],
            args.model_a_name,
            args.model_b_name
        )

        item["winner"] = winner
        counts[winner] += 1

        # 每 5 条保存一次，防止中断丢失
        if (i + 1) % 5 == 0:
            write_json(os.path.join(args.output_dir, "pairwise_results.json"), results)

        time.sleep(args.delay_between_calls)  # 控制请求频率

    # 保存最终结果
    final_results_path = os.path.join(args.output_dir, "pairwise_results.json")
    write_json(final_results_path, results)

    # 生成摘要
    total_valid = counts["A"] + counts["B"] + counts["C"]
    win_rate_a = counts["A"] / total_valid if total_valid else 0
    win_rate_b = counts["B"] / total_valid if total_valid else 0

    summary = {
        "model_a": args.model_a_name,
        "model_b": args.model_b_name,
        "total": total,
        "evaluated": total_valid,
        "win_counts": dict(counts),
        "win_rate_A": round(win_rate_a, 4),
        "win_rate_B": round(win_rate_b, 4),
        "tie_rate": round(counts["C"] / total_valid, 4) if total_valid else 0,
        "error_rate": round(counts["ERROR"] / total, 4)
    }

    summary_path = os.path.join(args.output_dir, "summary.json")
    write_json(summary_path, summary)

    print("\n" + "="*60)
    print("✅ Pairwise Evaluation Completed!")
    print(json.dumps(summary, indent=2, ensure_ascii=False))
    print("="*60)


if __name__ == "__main__":
    main()